---
title: "Stand Up Mining"
author: "Grigoryan Anna"
date: "5/20/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}

#install.packages("xml2")
#install.packages("rvest")
#install.packages("selectr")
#install.packages("stringr")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("tidytext")
#install.packages("tidyr")
#install.packages("scales")
#install.packages("wordcloud")
#install.packages("igraph")
#install.packages("ggraph")
#remotes::install_github("jooyoungseo/youtubecaption")
#install.packages("gridExtra")
#devtools::install_github("jaredhuling/jcolors")

library(xml2)
library(rvest)
library(selectr)
library(stringr)
library(dplyr)
library(ggplot2)
library(tidytext)
library(tidyr)
library(scales)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(youtubecaption)
library(gridExtra)
```


## Introduction

Humor is a universal phenomenon, however it can be expressed and accepted differently. Two wide-spread channels are Russian and English speaking standup comedians. 

Being a big fan of standup culture, I am conducting this analysis to show the sentiments behind the  standup monolugues. My aim is to understand the cross-cultrual differences and look for interesting patters.


## Data 

The data is obtained from the top 40 standup search results from https://www.youtube.com in English and in Russian. The text of the transcript is obtained by web-scraping, using the package "youtubecaption" (https://github.com/jooyoungseo/youtubecaption) from GitHub was used. For sentiment analysis,  "RuSentiLex" (http://www.labinform.ru/pub/rusentilex/index.htm) project dictionary was added as the Russian lexicon.


```{r include=FALSE, paged.print=TRUE}
urls_en <- c(
  "https://www.youtube.com/watch?v=G0eeNijdv3I",
  "https://www.youtube.com/watch?v=j3ySANXUkaw",
  "https://www.youtube.com/watch?v=D2eUomfJFcI&t=1s",
  "https://www.youtube.com/watch?v=G0eeNijdv3I",
  "https://www.youtube.com/watch?v=--IS0XiNdpk",
  "https://www.youtube.com/watch?v=Whde50AacZs",
  "https://www.youtube.com/watch?v=-mSGwndFMp8",
  "https://www.youtube.com/watch?v=en5_JrcSTcU",
  "https://www.youtube.com/watch?v=z2X0TaXknVE",
  "https://www.youtube.com/watch?v=4Lsa7N7LHJk",
  "https://www.youtube.com/watch?v=gyFEvjj7AwM",
  "https://www.youtube.com/watch?v=lK37j61MnPo",
  "https://www.youtube.com/watch?v=xcg_e-FY_Vs",
  "https://www.youtube.com/watch?v=oeZ93EaaTb8",
  "https://www.youtube.com/watch?v=e7T4uY6Wnao",
  "https://www.youtube.com/watch?v=qfb-T7rTWfs",
  "https://www.youtube.com/watch?v=tw81rmWzBNU",
  "https://www.youtube.com/watch?v=P-cKDFbCwrQ",
  "https://www.youtube.com/watch?v=L2vsv_SyEec",
  "https://www.youtube.com/watch?v=IvqMGlDRI9Q")


urls_ru <- c(
  "https://www.youtube.com/watch?v=ufT_Hhl22FA",
  "https://www.youtube.com/watch?v=GBWz6MDPGhI&t=80s",
  "https://www.youtube.com/watch?v=ufT_Hhl22FA&t=17s",
  "https://www.youtube.com/watch?v=s6khVgCaGJ4",
  "https://www.youtube.com/watch?v=aHRkJUCXNiI",
  "https://www.youtube.com/watch?v=laz34shu7L4",
  "https://www.youtube.com/watch?v=oQ777L_WwoI",
  "https://www.youtube.com/watch?v=cj-F6lluXiw",
  "https://www.youtube.com/watch?v=8OWFMYGP03I",
  "https://www.youtube.com/watch?v=gI6IMDiQCWU",
  "https://www.youtube.com/watch?v=uYf4sKP7m70",
  "https://www.youtube.com/watch?v=UZruK6KhoIw",
  "https://www.youtube.com/watch?v=HYob5A4AUzM&t=34s",
  "https://www.youtube.com/watch?v=bXQkkZjOMuk",
  "https://www.youtube.com/watch?v=3kkACsvu3n0",
  "https://www.youtube.com/watch?v=xrSJv5bgxHE&t=80s",
  "https://www.youtube.com/watch?v=6uc2xxCv5yE&t=132s",
  "https://www.youtube.com/watch?v=GxGk58pqNNk",
  "https://www.youtube.com/watch?v=ANd9CpMAgho",
  "https://www.youtube.com/watch?v=JbUPH3x2T_4")

urls <- urls_ru
captions_ru <- {}
#length(urls)
for (i in (1:length(urls))) {
  caption <- get_caption(urls[i])
  caption <- toString(caption$text)
  caption <- tolower(caption)
  caption <- str_replace_all(caption, "[[:punct:]]", "")
  caption <- str_replace_all(caption, "\n", " ")
  caption <- str_replace_all(caption, "♪♪♪", "")
  caption <- str_replace_all(caption, ">>", "")
  #print(i)
  #print(caption)
  captions_ru[i] <- caption
  }

urls <- urls_en
captions_en <- {}
#length(urls)
for (i in (1:length(urls))) {
  caption <- get_caption(urls[i])
  caption <- toString(caption$text)
  caption <- tolower(caption)
  caption <- str_replace_all(caption, "[[:punct:]]", "")
  caption <- str_replace_all(caption, "\n", " ")
  caption <- str_replace_all(caption, "♪", "")
  caption <- str_replace_all(caption, ">>", "")
  caption <- str_replace_all(caption, "  ", " ")
 #print(i)
 #print(caption)
  captions_en[i] <- caption
}
```
Here is small sample from the first monologues scraped from each search result.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
sample_en <- substr(captions_en[1], 1, 498)
print("Here is a small sample from the first monologue in English")
print(sample_en)

sample_ru <- substr(captions_ru[1], 1, 498)
print("Here is a small sample from the first monologue in Russian")
print(sample_ru)
```

In order to add the Russian words to the lexicon,we first need to format the dictionary into the same format as the BING sentiment is. The dictionary is loaded from the .csv file, which is downloaded from the RuSentiLex project website. After editing the columns and making the required changes, the words are added to the existing sentiments. For reference the dictionary.R file can be reviewed.

```{r message=FALSE, warning=FALSE, include=TRUE}
dictionary <- read.csv("dictionary.csv", header = TRUE)



```

```{r echo=TRUE, message=FALSE, warning=FALSE}
head(dictionary)
```
## Tidy text format 

First we need to edit the monologue text into  tidy text format, i.e. one-token-per-row. 
```{r echo=TRUE, message=FALSE, warning=FALSE}
text_en <- tibble(text=captions_en, monologue = 1:20)
tidy_text_en <- text_en %>%
  unnest_tokens(word, text)

text_ru <- tibble(text=captions_ru, monologue = 1:20)
tidy_text_ru <- text_ru %>%
  unnest_tokens(word, text)
```
Following tokenization, the stop words, which contain no meaning should be removed. For customization, some additional stop words based on the specifics of standup monologues have been added. The stop words in Russian are loaded from the "stopwords-ru.csv" file.

```{r echo=TRUE, message=FALSE, warning=FALSE}
data(stop_words)
custom_stop_words_en <- bind_rows(tibble(word = c("im", "laughter", "applause", "ladies", "gentlemen", "dont", "didnt", "cheers", "tonight", "ill", "id"), 
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words_ru <-read.csv("stopwords-ru.csv", header = FALSE)
colnames(custom_stop_words_ru)="word"

tidy_text_en <- tidy_text_en %>%
  anti_join(custom_stop_words_en)

tidy_text_ru <- tidy_text_ru %>%
  anti_join(custom_stop_words_ru)
```
The following graph shows the most common words in standups. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
tidy_text_en %>%
  count(word, sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(color="white", fill="royalblue") +
  ylab("number") +
  ggtitle("Number of times the word has appeared in 20 standups in English") +
  coord_flip()

tidy_text_ru %>%
  count(word, sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(color="white", fill="royalblue") +
  ylab("number") +
  ggtitle("Number of times the word has appeared in 20 standups in Russian") +
  coord_flip()
```


```{r message=FALSE, warning=FALSE, include=FALSE}
mono <- tidy_text_en %>%
  group_by(monologue) %>%
  summarise(count=n())

mono <- mono %>%
  mutate(cumsum = cumsum(mono$count))

tidy_text_en$wordnumber <- as.integer(row.names(tidy_text_en))
new <- left_join(tidy_text_en, mono)
new$dif <- new$cumsum-new$count
new$wordnumber=new$wordnumber-new$dif
new <- new[ -c(5,6) ]

monoru <- tidy_text_ru %>%
  group_by(monologue) %>%
  summarise(count=n())

monoru <- monoru %>%
  mutate(cumsum = cumsum(monoru$count))
monoru

tidy_text_ru$wordnumber <- as.integer(row.names(tidy_text_ru))


newru <- left_join(tidy_text_ru, monoru)
newru
newru$dif <- newru$cumsum-newru$count
newru$wordnumber=newru$wordnumber-newru$dif
newru <- newru[ -c(5,6) ]


```


## Sentiment analysis with tidy data

First, using the "bing" sentiment we can count up how many positive and negative words there are in defined sections of each monologue. We define an index here to keep track of where we are in the monologue; this index (using integer division) counts up sections of 10 words of text. The underlying assumption is that a sentence has on average 10 words. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

text_en_sentiment <- new %>%
  inner_join(get_sentiments("bing")) %>%
  count(monologue, index = wordnumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(text_en_sentiment, aes(index, sentiment, fill = monologue)) +
  geom_col(show.legend = FALSE) 


text_ru_sentiment <- newru %>%
  left_join(dictionary) 

text_ru_sentiment <- text_ru_sentiment[!is.na(text_ru_sentiment$sentiment),]


text_ru_sentiment <- text_ru_sentiment %>%
  count(monologue, index = wordnumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(text_ru_sentiment, aes(index, sentiment, fill = monologue)) +
  geom_col(show.legend = FALSE) 
```
In English monologues where the sentiments are more intense in the beginning, sentiments are less intense close to the end. The opposite is also true: the monolugues starting with moderatesentiments are still intese close to the end. In the Russian monologues this is less observable.

We now have an estimate of the net sentiment (positive - negative) in each chunk of words for each sentiment lexicon. If we bind them together and visualize we can see that the three different lexicons give results that are different in an absolute sense but have similar relative trajectories through the monologues. 


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
afinn <- new %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = wordnumber %/% 10) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")


bing_and_nrc <- bind_rows(new %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          new %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = wordnumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
We see similar dips and peaks in sentiment at about the same places, but the absolute values are significantly different. The AFINN lexicon gives the largest absolute values, with high positive values. The lexicon from Bing et al. has lower absolute values and seems to label larger blocks of contiguous positive or negative text. The NRC results are shifted higher relative to the other two, labeling the text more positively, but detects similar relative changes in the text.

Now we can analyze word counts that contribute to each sentiment. By implementing count() and finding out how much each word contributed to each sentiment. This can be shown visually.


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

bing<- newru %>% 
                            inner_join(dictionary) %>%
                            mutate(method = "Bing et al.")%>%
  count(method, index = wordnumber %/% 10, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bing %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
dictionary %>% 
  count(sentiment)
```
The sentiments from the Russian standup monoligues are more negative. Which is quite explained by the fact that the lexicon used contains more negative words.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
bing_word_counts <- new %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
         


```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
bing_word_counts <- newru%>%
  inner_join(dictionary) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
         


```

In contrast with the words English the ones from standups in Russian are strictly negative and strictly positive, like awful, horrow, happiness, devitedness, etc. 

## Wordclouds

The size of a word’s text is proportional to its frequency within its sentiment. We can use this visualization to see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
tidy_text_en%>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("darkred", "darkgreen"),
                   max.words = 75)


tidy_text_ru%>%
  inner_join(dictionary) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("darkred", "darkgreen"),
                   max.words = 75)
```






The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common. The bind_tf_idf function in the tidytext package takes a tidy text dataset as input with one row per token (term), per document. One column (word here) contains the terms/tokens, one column contains the documents (monologue in this case), and the last necessary column contains the counts, how many times each monologue contains each term (n in this example). We calculated a total for each book for our explorations in previous sections, but it is not necessary for the bind_tf_idf function; the table only needs to contain all the words in each document.


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
monologue_words <- text_en %>%
  unnest_tokens(word, text) %>%
  count(monologue, word, sort = TRUE)

total_words <- monologue_words %>% 
  group_by(monologue) %>% 
  summarize(total = sum(n))

words <- left_join(monologue_words, total_words)

#ggplot(words, aes(n/total)) + geom_histogram(show.legend = FALSE, bins = 50, color="royalblue", fill="royalblue") 

freq_by_rank <- words %>% 
  group_by(monologue) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

#freq_by_rank %>% 
  #ggplot(aes(rank, `term frequency`, color = monologue)) + 
  #geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  #scale_x_log10() +
  #scale_y_log10()


words <- words %>%
  bind_tf_idf(word, monologue, n)


words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(monologue) %>% 
  top_n(1) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = monologue)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
 # facet_wrap(~monologue, ncol = 2, scales = "free") +
  coord_flip()

```
These words are, as measured by tf-idf, the most important to each monologue. 
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
monologue_words <- text_ru %>%
  unnest_tokens(word, text) %>%
  count(monologue, word, sort = TRUE)

total_words <- monologue_words %>% 
  group_by(monologue) %>% 
  summarize(total = sum(n))

words <- left_join(monologue_words, total_words)

#ggplot(words, aes(n/total)) + geom_histogram(show.legend = FALSE, bins = 50, color="royalblue", fill="royalblue") 

freq_by_rank <- words %>% 
  group_by(monologue) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

#freq_by_rank %>% 
  #ggplot(aes(rank, `term frequency`, color = monologue)) + 
  #geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  #scale_x_log10() +
  #scale_y_log10()


words <- words %>%
  bind_tf_idf(word, monologue, n)


words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(monologue) %>% 
  top_n(1) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = monologue)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
 # facet_wrap(~monologue, ncol = 2, scales = "free") +
  coord_flip()

```
In the last diagram we can observe that Russian standup is less censored. It has words suchs as war, death, drugs, etc. 


## Biagrams

The bigrams “not like” and “not help” were overwhelmingly the largest causes of misidentification, making the text seem much more positive than it is. But we can see phrases like “not afraid” and “not fail” sometimes suggest text is more negative than it is. “Not” isn’t the only term that provides some context for the following word. We could pick four common words (or more) that negate the subsequent term, and use the same joining and counting approach to examine all of them at once.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

tidy_text_en_bigram <- text_en %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


bigrams_separated_en <- tidy_text_en_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_en <- bigrams_separated_en %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts_en <- bigrams_filtered_en %>% 
  count(word1, word2, sort = TRUE)

bigrams_united_en<- bigrams_filtered_en %>%
  unite(bigram, word1, word2, sep = " ")

bigram_tf_idf_en <- bigrams_united_en %>%
  count(monologue, bigram) %>%
  bind_tf_idf(bigram, monologue, n) %>%
  arrange(desc(tf_idf))



AFINN <- get_sentiments("afinn")



negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated_en %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE)

negated_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not, no, never, without \"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()


```



We can clearly see that even the racist term has been counted as negative, in some cases it took the not close too, making the direction of the system completely oposite. With the same analogy, the word good is overvalued as it takes negating terms too. 



```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
tidy_text_ru_bigram <- text_ru %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


bigrams_separated_ru <- tidy_text_ru_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_ru <- bigrams_separated_ru %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts_ru <- bigrams_filtered_ru %>% 
  count(word1, word2, sort = TRUE)

bigrams_united_ru <- bigrams_filtered_ru %>%
  unite(bigram, word1, word2, sep = " ")

bigram_tf_idf_ru <- bigrams_united_ru %>%
  count(monologue, bigram) %>%
  bind_tf_idf(bigram, monologue, n) %>%
  arrange(desc(tf_idf))


negation_words <- c("не", "нет", "ни")
negated_words <- bigrams_separated_ru %>%
  filter(word1 %in% negation_words) %>%
  inner_join(dictionary, by = c(word2 = "word")) %>%
  count(word1, word2, score,  sort = TRUE)


negated_words %>%
  mutate(contribution = n*score) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n*score, fill = n*score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"нет, не, ни \"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
bigram_graph_en <- bigram_counts_en %>%
  filter(n > 3) %>%
  graph_from_data_frame()


set.seed(2017)


a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph_en, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```
```{r}
bigram_graph_ru <- bigram_counts_ru %>%
  filter(n > 15) %>%
  graph_from_data_frame()


set.seed(2017)


a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph_ru, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```



Through the graphs above we can visualize some details of the text structure, this helps to understand which words are just common in speech due to language structure, and which make sense. 




## Conclusion 

By conducting this text mining on standup monologues I understood that indeed the sentiments are different. To my surprise, the Russian standups had more sensitive keyterms, such as pregnant, shame, drugs, etc. 






